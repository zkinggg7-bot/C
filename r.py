import os
import google.generativeai as genai
from dotenv import load_dotenv
import sys
from PIL import Image
import glob
import json

# --- ุงููุณุงุฑุงุช ุงูุฏููุงููููุฉ ---
# ุณูุชู ุชุญุฏูุฏูุง ุจูุงุกู ุนูู ุงุณู ุงููุดุฑูุน
PROJECT_NAME = None
BASE_PROJECTS_DIR = "projects"
INPUT_FOLDER = None
OUTPUT_FOLDER = None
GLOSSARY_FILE = None
FINAL_FILENAME = "full_chapter_translation.txt"

def setup_project_paths(project_name):
    """ุฅุนุฏุงุฏ ุงููุณุงุฑุงุช ุจูุงุกู ุนูู ุงุณู ุงููุดุฑูุน ุงูููุนุทู."""
    global PROJECT_NAME, INPUT_FOLDER, OUTPUT_FOLDER, GLOSSARY_FILE
    PROJECT_NAME = project_name
    project_dir = os.path.join(BASE_PROJECTS_DIR, PROJECT_NAME)
    
    INPUT_FOLDER = os.path.join(project_dir, "input_images")
    OUTPUT_FOLDER = os.path.join(project_dir, "output_translations")
    GLOSSARY_FILE = os.path.join(project_dir, "glossary.json")
    
    # ุฅูุดุงุก ุงููุฌูุฏุงุช ุงูุถุฑูุฑูุฉ ูููุดุฑูุน ุฅุฐุง ูู ุชูู ููุฌูุฏุฉ
    os.makedirs(INPUT_FOLDER, exist_ok=True)
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)
    print(f"๐๏ธ ุชู ุชุญุฏูุฏ ุงููุดุฑูุน: '{PROJECT_NAME}'. ุณูุชู ุงุณุชุฎุฏุงู ุงููุณุงุฑุงุช ุฏุงุฎู ูุฌูุฏู ุงูุฎุงุต.")

def load_glossary():
    if not os.path.exists(GLOSSARY_FILE):
        with open(GLOSSARY_FILE, 'w', encoding='utf-8') as f:
            json.dump({}, f)
        return {}
    try:
        with open(GLOSSARY_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (json.JSONDecodeError, FileNotFoundError):
        return {}

def save_glossary(glossary_data):
    with open(GLOSSARY_FILE, 'w', encoding='utf-8') as f:
        json.dump(glossary_data, f, ensure_ascii=False, indent=4)

# --- ุจุงูู ุงูุฏูุงู (translate_image, find_and_update_new_terms) ุชุจูู ููุง ูู ---
def translate_image(image_path, model, glossary, previous_page_translation):
    print(f"\n--- โณ ุฌุงุฑู ูุนุงูุฌุฉ ุงูุตูุฑุฉ: {os.path.basename(image_path)} ---")
    img = Image.open(image_path)
    glossary_text = "\n".join([f"- {k}: {v}" for k, v in glossary.items()])
    context_section = ""
    if previous_page_translation:
        context_section = f"""
**ุงููุงุนุฏุฉ ุงูุซุงููุฉ: ุงูุชุจู ููุณูุงู ุงููุงูู!**
ูุฐู ูู ุงูุชุฑุฌูุฉ ุงููุงููุฉ ููุตูุญุฉ ุงูุณุงุจูุฉ. ุงุณุชุฎุฏููุง ูุณูุงู ูููู ุงููุตุฉ ูุชุฑุฌูุฉ ุงูุตูุญุฉ ุงูุญุงููุฉ ุจุดูู ุตุญูุญ:
--- ุจุฏุงูุฉ ุณูุงู ุงูุตูุญุฉ ุงูุณุงุจูุฉ ---
{previous_page_translation}
--- ููุงูุฉ ุณูุงู ุงูุตูุญุฉ ุงูุณุงุจูุฉ ---
"""
    translation_prompt = f"""
ุฃูุช ุฎุจูุฑ ููุชุฑุฌู ูุงูููุง ูุญุชุฑู. ุงูุธุฑ ุฅูู ุงูุตูุฑุฉ ุงููุฑููุฉ ููู ุจูุง ููู:
**ุงููุงุนุฏุฉ ุงูุฃููู ูุงูุฃุณุงุณูุฉ: ุงูุชุฒู ุจุงููุณุฑุฏ ุงูุชุงูู ุจุดูู ุฅูุฒุงูู:**
--- ุจุฏุงูุฉ ุงููุณุฑุฏ ---
{glossary_text}
--- ููุงูุฉ ุงููุณุฑุฏ ---
{context_section}
**ุจููุฉ ุงูููุงุนุฏ:**
ูู ููุท ุจูุง ูู ูุทููุจ ุฃุฏูุงู ุจุฏูู ุฅุถุงูุฉ ุฃู ููุงู ุฒุงุฆุฏ ุฃู ุชูุถูุญุงุช ุฅุถุงููุฉุ ููุง ุชุณุชุฎุฏู ุฃู ุชูุณููุงุช ูุซู ุงููุฌูู ุฃู ุงููุงุฑูุฏุงูู ุฃู ุฃู ุดูู ุขุฎุฑ ูู ุฃุดูุงู ุงูุชูุณูู ูู ุฅุฌุงุจุชู.

ุงุณุชุฎุฑุฌ ุงููุต ุงูููุฑู ุฃู ุงูุฅูุฌููุฒู ุงูููุฌูุฏ ุฏุงุฎู ูู ููุงุนุฉ ุญูุงุฑ ุจุดูู ูููุตู. ูุง ุชุฏูุฌ ุงููุตูุต ูู ููุงุนุงุช ูุฎุชููุฉ. ุงูููุงุนุฉ ุชูุชุฑุฌู ูุงููุฉ ุจุฏูู ูุตู ููุง ุชุฏูุฌ ุจูู ุงูููุงุนุงุช.

ุชุฑุฌู ูู ูุต ููุฑู ุฃู ุฅูุฌููุฒู ุฅูู ุงููุบุฉ ุงูุนุฑุจูุฉ ุจุงูุฎุทูุงุช ุงูุชุงููุฉ ูุจุงูุชุฑุชูุจ:

ุงูููุฑูุฉ ุฃู ุงูุฅูุฌููุฒูุฉ: ุงูุชุจ ุงููุต ุงูููุฑู ุงูุฃุตูู ุฃู ุงูุฅูุฌููุฒู ุงูุฐู ุงุณุชุฎุฑุฌุชู.
ุงูุชุฑุฌูุฉ ุงูุญุฑููุฉ: ุงูุชุจ ุงูุชุฑุฌูุฉ ุงูุญุฑููุฉ ุฏูู ูุทุน.
ุงูุชุฑุฌูุฉ ุงูุฃุฏุจูุฉ ุงูุนุฑุจูุฉ: ุงูุชุจ ุชุฑุฌูุฉ ุฃุฏุจูุฉ ุนูููุฉ ุตุญูุญุฉ ูุญูููุง ููุบูููุงุ ูุน ุงุณุชุฎุฏุงู ุนูุงูุงุช ุงูุชุฑููู ุงูููุงุณุจุฉ ูุงููุตุทูุญุงุช ุงูุดุงุฆุนุฉ ูุฅูุตุงู ุงููุนูู ุจุฃูุถู ุดูู ูููุงุฑุฆ ุงูุนุฑุจู.

ุฅุฐุง ูุงูุช ุงูุตูุฑุฉ ูุจูุฑุฉ ููููุง ุฃูุซุฑ ูู ููุงุนุฉ ุงุฌุนู ุงูุชุฑุฌูุฉ ููุงุณุจุฉ ููุณูุงู. ุชุฑุฌู ููุท ุงูููุงุนุงุช ููุง ุชุชุฑุฌู ุงูุฃุตูุงุช.
ุฅุฐุง ูุฌุฏุช ูุตูุง ุฎุงุฑุฌ ููุงุนุงุช ุงูุญูุงุฑุ ูู ุจุชุฑุฌูุชู ุจููุณ ุงูุฎุทูุงุช ุฃุนูุงู ููู ุฃุถู ูู ุจุฏุงูุฉ ุงูุณุทุฑ ุนุจุงุฑุฉ ยซุชูุจูู: ูุต ุฎุงุฑุฌ ููุงุนุฉยป.
ุงุฌุนู ุงููุฎุฑุฌ ุงูููุงุฆู ููุธููุง ูููุฌููุง ูู ุงููููู ุฅูู ุงููุณุงุฑ ูุจุฏูู ุฃู ุชูุณูู ุฃู ุนูุงูุงุช ุฎุงุตุฉ.
    """
    response = model.generate_content([translation_prompt, img])
    return response.text

def find_and_update_new_terms(text_to_analyze, model, glossary):
    print("--- ๐ง ุงูุจุญุซ ุนู ูุตุทูุญุงุช ุฌุฏูุฏุฉ ูุชุญุฏูุซ ุงููุณุฑุฏ...")
    extraction_prompt = f"""
    ุฃูุช ูุณุงุนุฏ ูุชุฎุตุต ูู ุชุญููู ุงููุตูุต. ุงูุธุฑ ุฅูู ุงููุต ุงูุชุงูู.
    ูู ูุญุชูู ุงููุต ุนูู ุฃู ุฃุณูุงุก ุดุฎุตูุงุชุ ุฃูุงููุ ุฃู ูุตุทูุญุงุช ูููุฉ ูู ุชูู ููุฌูุฏุฉ ูู ุงููุณุฑุฏ ุงูุฃุตููุ
    **ุงููุณุฑุฏ ุงูุฃุตูู:** {list(glossary.keys())}
    **ุงููุต ููุชุญููู:** {text_to_analyze}
    **ุงููุทููุจ:** ุฅุฐุง ูุฌุฏุช ูุตุทูุญุงุช ุฌุฏูุฏุฉุ ุฃุฑุฌุนูุง ููุท ุจุชูุณูู JSON. ุฅุฐุง ูู ุชุฌุฏ ุดูุฆูุงุ ุฃุฑุฌุน {{}}.
    ูุซุงู: {{"New Term 1": "Translation 1", "New Term 2": "Translation 2"}}
    """
    response = model.generate_content(extraction_prompt)
    try:
        clean_response = response.text.strip().replace('```json', '').replace('```', '')
        new_terms = json.loads(clean_response)
        if new_terms:
            updated_count = 0
            for term, translation in new_terms.items():
                if term not in glossary:
                    glossary[term] = translation
                    updated_count += 1
            if updated_count > 0:
                print(f"โ ุชู ุงูุนุซูุฑ ุนูู {updated_count} ูุตุทูุญ ุฌุฏูุฏ ูุฅุถุงูุชูุง ูููุณุฑุฏ.")
                save_glossary(glossary)
            else:
                print("--- ูู ูุชู ุงูุนุซูุฑ ุนูู ูุตุทูุญุงุช ุฌุฏูุฏุฉ.")
        else:
            print("--- ูู ูุชู ุงูุนุซูุฑ ุนูู ูุตุทูุญุงุช ุฌุฏูุฏุฉ.")
    except (json.JSONDecodeError, AttributeError) as e:
        print(f"[ุชุญุฐูุฑ] ูู ูุชููู ูู ุชุญููู ุงููุตุทูุญุงุช ุงูุฌุฏูุฏุฉ. ุงูุฎุทุฃ: {e}")

def main():
    # --- ุงูุชุญูู ูู ูุฌูุฏ ุงุณู ุงููุดุฑูุน ---
    if len(sys.argv) < 2:
        print("โ ุฎุทุฃ: ูุฑุฌู ุชุญุฏูุฏ ุงุณู ุงููุดุฑูุน ุนูุฏ ุชุดุบูู ุงูุณูุฑุจุช.")
        print("ูุซุงู: python3 run_translator.py solo-leveling")
        sys.exit(1)
    
    project_name_arg = sys.argv[1]
    setup_project_paths(project_name_arg)

    # --- ุจููุฉ ุงูุฏุงูุฉ main ุชุนูู ูุงูุณุงุจู ูููู ุนูู ุงููุณุงุฑุงุช ุงูุฌุฏูุฏุฉ ---
    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("[ุฎุทุฃ] ูู ูุชู ุงูุนุซูุฑ ุนูู ููุชุงุญ GOOGLE_API_KEY.")
        sys.exit(1)
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.5-flash')
    print("โ Gemini API ุฌุงูุฒ ููุนูู.")

    glossary = load_glossary()
    print(f"โ ุชู ุชุญููู ุงููุณุฑุฏ ุงูุฎุงุต ุจุงููุดุฑูุน '{PROJECT_NAME}' ููุญุชูู ุนูู {len(glossary)} ูุตุทูุญ.")

    image_paths = sorted(glob.glob(os.path.join(INPUT_FOLDER, '*.*')))
    if not image_paths:
        print(f"ูู ูุชู ุงูุนุซูุฑ ุนูู ุฃู ุตูุฑ ูู ุงููุฌูุฏ '{INPUT_FOLDER}'.")
        sys.exit(0)
    print(f"โ ุชู ุงูุนุซูุฑ ุนูู {len(image_paths)} ุตูุฑุฉ. ุณุชุจุฏุฃ ุนูููุฉ ุงูุชุฑุฌูุฉ...")

    all_translations_for_final_file = []
    previous_page_full_translation = None

    for path in image_paths:
        translation = translate_image(path, model, glossary, previous_page_full_translation)
        
        base_name = os.path.basename(path)
        file_name_without_ext = os.path.splitext(base_name)[0]
        output_path = os.path.join(OUTPUT_FOLDER, f"{file_name_without_ext}.txt")
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(translation)
        print(f"โ ุชู ุญูุธ ุงูุชุฑุฌูุฉ ุงููููุตูุฉ ูู: {output_path}")
        
        find_and_update_new_terms(translation, model, glossary)
        
        previous_page_full_translation = translation
        print(f"--- โ ุชู ุญูุธ ุณูุงู ุงูุตูุญุฉ ุงููุงููุฉ ูููุฑุฉ ุงููุงุฏูุฉ.")
        
        page_separator = f"\n\n--- ููุงูุฉ ุชุฑุฌูุฉ ุตูุญุฉ: {base_name} ---\n\n"
        all_translations_for_final_file.append(translation + page_separator)

    final_output_path = os.path.join(OUTPUT_FOLDER, FINAL_FILENAME)
    with open(final_output_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(all_translations_for_final_file))
    
    print(f"\nโ ุชู ุญูุธ ุงูููู ุงููุฌูุน ูููุตู ุจุงููุงูู ูู: {final_output_path}")
    print("\n๐๐๐ ุงูุชููุช ุชุฑุฌูุฉ ุฌููุน ุงูุตูุฑ ุจูุฌุงุญ! ๐๐๐")

if __name__ == "__main__":
    main()
